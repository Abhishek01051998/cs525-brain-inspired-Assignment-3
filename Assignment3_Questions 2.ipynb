{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "In Assignment 2, we learnt how to construct networks of spiking neurons and propagate information through a network of fixed weights. In this assignment, you will learn how to train network weights for a given task using brain-inspired learning rules.\n",
    "\n",
    "Let's import all the libraries required for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Training a Network\n",
    "\n",
    "## 1a. \n",
    "What is the purpose of a learning algorithm? In other words, what does a learning algorithm dictate, and what is the objective of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a. \n",
    "A learning algorithm is a set of instructions used in machine learning that allows a computer program to imitate how a human gets better at characterizing some types of information.\n",
    "These Learning algorithms are not explainable as only the program knows the specific cognitive shortcuts towards finding the best solution. The algorithm considers all the variables it has been exposed to during its training and finds the best combination of these variables to solve a problem. This unique combination of variables is ‘learned’ by the machine through trial and error. There are many types of machine learning based on the kind of training.\n",
    " \n",
    "Thus, it is easy to see how machine learning algorithms can be helpful in situations where a lot of data is present. The more data that an ML algorithm ingests, the more effective it can be at solving the problem at hand. The program continues to improve and iterate upon itself every time it solves the problem.\n",
    "\n",
    "The purpose of  learning Algorithms  is to discover patterns in your data and then make predictions based on often complex patterns to answer business questions, detect and analyse trends and help solve problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "Categorize and explain the various learning algorithms w.r.t. biological plausibility. Can you explain the tradeoffs involved with the different learning rules? *Hint: Think computational advantages and disadvantages of biological plausibility.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Supervised Learning, the desired signal is available for the network to learnfrom. This desired signal is also called as a teaching signal, as it instructs thenetwork to predict accurately. A variety of brain functions are associated withsupervised learning: sensorimotor networks that control goal-directed behaviours,such as reaching out for an object, are calibrated based on sensory feedback4344 . Theearliest and the most straightforward learning algorithm is Gradient Descent.13 Itis an error based learning algorithm where the network traverses the error surface insearch of a minima. The BPA is a manifestation of the gradient descent optimisationin which the error is propagated back into the network leading to weight updation.Though GD computationally makes sense, there is no evidence of such a processoccurring in the brain.\n",
    "Reinforcement Learning\n",
    "Reinforcement learning is a learning paradigm that uses reward and punish-ments to learn. This forms the basis of learning motor functions in the brain,especially the basal ganglia and frontal cortex. Reinforcement learning also playsa major role in animal cognition as evidenced by Pavlovian conditioning. For ourpurposes we restrict our analysis to supervised and unsupervised learning algorithms.\n",
    " unsupervised learning, \n",
    " the desired output is unavailable. Therefore there is noteaching signal to gear the network towards the target. Identifying familiar visualforms points to rapid unsupervised learning in the brain. Learning happens whenthe network is exposed to the data and reacts. Repeated exposures to the data leadto certain connections or weight changes emerging in the network which enables itto classify a new signal.The Hebbian rule provides a more biologically plausible algorithm for learning. Itstates that the connections between neurons are strengthened when their spiking issynchronised and vice-versa. Spike Time Dependent Plasticity (STDP) and Con-trastive Hebbian Learning (CHL)are two implementations of the Hebbian rulewhich shall be dealt in the later sections.Bayesian Conﬁdence Propagation Neural Network (BCPNN) is another example ofthe Hebbian learning rule where the connection between two neurons are strength-ened when the probability of them spiking together is high. Additionally, the neuronsare arranged in mini-columns and hyper-columns in accordance to the arrangementof the neurons in the cerebral cortex.Neuro-Evolution of Augmenting Topology (NEAT) trains an artiﬁcial neural net-work through genetic algorithm to emulate the formation of the brain through bio-logical evolution. By evolving diﬀerent topologies of the network through crossoversand eliminating the weak performers through the ﬁtness function, the network op-timises its performance. From these algorithms, I have selected four based on theirpopularity and biological plausibility. They will be described in detail in the nextsection.\n",
    " \n",
    "Speaking about their biological plausibility, we  can tell that :\n",
    "Unsupervised learning has passive changes that exploits statsitical coorelations and it is useful for development i.e. wiring for receptive fields.\n",
    "Reinforcement learning has conditioned changes that maximizes the rewards. That is useful for learning new behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Hebbian Learning\n",
    "\n",
    "## 2a.\n",
    "\n",
    "In this exercise, you will implement the hebbian learning rule to solve AND Gate. First, we need to create a helper function to generate the training data. The function should return lists of tuples where each tuple comprises of numpy arrays of rate-coded inputs and the corresponding rate-coded output. \n",
    "\n",
    "Below is the function to generate the training data. Fill the components to return the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genANDTrainData(snn_timestep):\n",
    "    \"\"\" \n",
    "    Function to generate the training data for AND \n",
    "        Args:\n",
    "            snn_timestep (int): timesteps for SNN simulation\n",
    "        Return:\n",
    "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
    "        \n",
    "        Write the expressions for encoding 0 and 1. Then append all 4 cases of AND gate to the list train_data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize an empty list for train data\n",
    "    train_data = []\n",
    "    \n",
    "    #encode 0. Numpy random choice function might be useful here. \n",
    "    zero = np.random.choice([1,0], snn_timestep, p=[0.3, 0.7])\n",
    "    \n",
    "    #encode 1. Numpy random choice function might be useful here. \n",
    "    one = np.random.choice([1,0], snn_timestep, p=[0.7, 0.3])\n",
    "    \n",
    "    #Append all 4 cases of AND gate to train_data. Numpy stack operation might be useful here. \n",
    "    i1 = np.array([zero,zero])\n",
    "    i2 = np.array([zero,one])\n",
    "    i3 = np.array([one,zero])\n",
    "    i4 = np.array([one,one])\n",
    "    \n",
    "    case_1 = np.array([i1,zero])\n",
    "    case_2 = np.array([i2,zero])\n",
    "    case_3 = np.array([i3,zero])\n",
    "    case_4 = np.array([i4,one])\n",
    "    train_data = np.stack((case_1, case_2, case_3, case_4))\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[array([[0, 0, 0, 1, 0],\n",
      "         [0, 0, 0, 1, 0]]) array([0, 0, 0, 1, 0])]\n",
      " [array([[0, 0, 0, 1, 0],\n",
      "         [0, 1, 1, 0, 1]]) array([0, 0, 0, 1, 0])]\n",
      " [array([[0, 1, 1, 0, 1],\n",
      "         [0, 0, 0, 1, 0]]) array([0, 0, 0, 1, 0])]\n",
      " [array([[0, 1, 1, 0, 1],\n",
      "         [0, 1, 1, 0, 1]]) array([0, 1, 1, 0, 1])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_1 = np.array([i1,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_2 = np.array([i2,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_3 = np.array([i3,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_4 = np.array([i4,one])\n"
     ]
    }
   ],
   "source": [
    "a=genANDTrainData(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. \n",
    "We will use the implementation of the network from assignment 2 to create an SNN comprising of one input layer and one output layer. Can you explain algorithmically, how you can use this simple architecture to learn AND gate. Your algorithm should comprise of encoding, forward propagation, network training, and decoding steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm consists of :\n",
    "\n",
    " Network Architecture: AND Gate implementation is a linearly separable function. Hence we do not need a hidden layer. We can implement it with the input and output layers. We will have two neurons in the input layer and 1 in the output layer.\n",
    "\n",
    "Spike-Based Encoding(Rate Encoding): We use spike-based Encoding to represent 0 and 1, and we also train the data set by feeding the output of and gate for the given input.\n",
    "\n",
    "Training the network:\n",
    "a) Here, we initialize the network with random weights.\n",
    "\n",
    "b) We iterate over all the samples in the training data set and compute the mean firing rate\n",
    "\n",
    "c) we update the weight using the learning formula\n",
    "\n",
    "d) We run this for a certain number of epochs such that there is no convergence\n",
    "\n",
    "Decoding: We decode the spike output using the threshold voltage value\n",
    "\n",
    "Testing Phase: Here, we will have a network with learned weights. So we iterate over the train data set with only inputs, compute the network output, decode it to the predicted class, and compare it with its label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNN has already been implemented for you. You do not need to do anything here. Just understand the implementation so that you can use it in the later parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeurons:\n",
    "    \"\"\" \n",
    "        Define Leaky Integrate-and-Fire Neuron Layer \n",
    "        This class is complete. You do not need to do anything here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimension, vdecay, vth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): Number of LIF neurons in the layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vdecay = vdecay\n",
    "        self.vth = vth\n",
    "\n",
    "        # Initialize LIF neuron states\n",
    "        self.volt = np.zeros(self.dimension)\n",
    "        self.spike = np.zeros(self.dimension)\n",
    "    \n",
    "    def __call__(self, psp_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_input (ndarray): synaptic inputs \n",
    "        Return:\n",
    "            self.spike: output spikes from the layer\n",
    "                \"\"\"\n",
    "        self.volt = self.vdecay * self.volt * (1. - self.spike) + psp_input\n",
    "        self.spike = (self.volt > self.vth).astype(float)\n",
    "        return self.spike\n",
    "\n",
    "class Connections:\n",
    "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
    "\n",
    "    def __init__(self, weights, pre_dimension, post_dimension):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (ndarray): connection weights\n",
    "            pre_dimension (int): dimension for pre-synaptic neurons\n",
    "            post_dimension (int): dimension for post-synaptic neurons\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.pre_dimension = pre_dimension\n",
    "        self.post_dimension = post_dimension\n",
    "    \n",
    "    def __call__(self, spike_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n",
    "        Return:\n",
    "            psp: postsynaptic layer activations\n",
    "        \"\"\"\n",
    "        psp = np.matmul(self.weights, spike_input)\n",
    "        return psp\n",
    "    \n",
    "    \n",
    "class SNN:\n",
    "    \"\"\" Define a Spiking Neural Network with No Hidden Layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_2_output_weight, \n",
    "                 input_dimension=2, output_dimension=2,\n",
    "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer\n",
    "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer\n",
    "            input_dimension (int): input dimension\n",
    "            hidden_dimension (int): hidden_dimension\n",
    "            output_dimension (int): output_dimension\n",
    "            vdecay (float): voltage decay of LIF neuron\n",
    "            vth (float): voltage threshold of LIF neuron\n",
    "            snn_timestep (int): number of timesteps for inference\n",
    "        \"\"\"\n",
    "        self.snn_timestep = snn_timestep\n",
    "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
    "        self.input_2_output_connection = Connections(input_2_output_weight, input_dimension, output_dimension)\n",
    "    \n",
    "    def __call__(self, spike_encoding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_encoding (ndarray): spike encoding of input\n",
    "        Return:\n",
    "            spike outputs of the network\n",
    "        \"\"\"\n",
    "        spike_output = np.zeros(self.output_layer.dimension)\n",
    "        for tt in range(self.snn_timestep):\n",
    "            input_2_output_psp = self.input_2_output_connection(spike_encoding[:, tt])\n",
    "            output_spikes = self.output_layer(input_2_output_psp)\n",
    "            spike_output += output_spikes\n",
    "        return spike_output/self.snn_timestep      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. \n",
    "Next, you need to write a function for network training using hebbian learning rule. The function is defined below. You need to fill in the components so that the network weights are updated in the right manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hebbian(network, train_data, lr=1e-5, epochs=10):\n",
    "    \"\"\" \n",
    "    Function to train a network using Hebbian learning rule\n",
    "        Args:\n",
    "            network (SNN): SNN network object\n",
    "            train_data (list): training data \n",
    "            lr (float): learning rate\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples. \n",
    "        \n",
    "        Write the operations required to compute the weight increment according to the hebbian learning rule. Then increment the network weights. \n",
    "    \"\"\"\n",
    "    \n",
    "    #iterate over the epochs\n",
    "    for ee in range(epochs):\n",
    "        w = np.zeros((1,2))\n",
    "        #iterate over all samples in train_data\n",
    "        for data in train_data:\n",
    "            \n",
    "            #compute the firing rate for the input\n",
    "            r1 = np.sum(data[0][0])/len(data[0][0])\n",
    "            r2 = np.sum(data[0][1])/len(data[0][1])\n",
    "\n",
    "            #compute the firing rate for the output\n",
    "            ro = np.sum(data[1])/len(data[1])\n",
    "\n",
    "            #compute the correlation using the firing rates calculated above\n",
    "            cor1 = r1 * ro\n",
    "            cor2 = r2 * ro\n",
    "\n",
    "            #compute the weight increment\n",
    "            w[0][0] = lr * cor1\n",
    "            w[0][1] = lr * cor2\n",
    "            \n",
    "            #increment the weight\n",
    "            network.input_2_output_connection.weights += w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. \n",
    "In this exercise, you will use your implementations above to train an SNN to learn AND gate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights  [[0.07757163 0.06727002]]\n",
      "Testing \n",
      "zero  [0 0 0 1 0]\n",
      "one  [0 1 1 1 0]\n",
      "case One - output  [0.2]\n",
      "case Two - output  [0.4]\n",
      "case Three - output  [0.4]\n",
      "case Four - output  [0.6]\n",
      "Final Output   [0, 0, 0, 1]\n",
      "Final Network Weights  [[0.44557163 0.43527002]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/3475826507.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_1 = np.array([i1,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/3475826507.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_2 = np.array([i2,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/3475826507.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_3 = np.array([i3,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/3475826507.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_4 = np.array([i4,one])\n"
     ]
    }
   ],
   "source": [
    "#Define a variable for input dimension\n",
    "input_dimension = 2\n",
    "\n",
    "#Define a variable for output dimension\n",
    "output_dimension = 1\n",
    "\n",
    "#Define a variable for voltage decay\n",
    "vdecay = 0.5\n",
    "\n",
    "#Define a variable for voltage threshold\n",
    "vth = 0.5\n",
    "\n",
    "#Define a variable for snn timesteps\n",
    "snn_timestep = 5\n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension) / 10\n",
    "\n",
    "#print the initial weights\n",
    "print(\"initial weights \", input_2_output_weight)\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "network_1 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Get the training data for AND gate using the function defined in 2a. \n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "#Train the network using the function defined in 2c. with the appropriate arguments\n",
    "hebbian(network_1, train_data, lr=2e-2, epochs=20)\n",
    "\n",
    "#Test the trained network and print the network output for all 4 cases.\n",
    "print(\"Testing \")\n",
    "\n",
    "zero = np.random.choice([1,0], snn_timestep, p=[0.3, 0.7])\n",
    "print(\"zero \", zero)\n",
    "\n",
    "one = np.random.choice([1,0], snn_timestep, p=[0.7, 0.3])\n",
    "print(\"one \", one)\n",
    "    \n",
    "i1 = np.array([zero,zero])\n",
    "i2 = np.array([zero,one])\n",
    "i3 = np.array([one,zero])\n",
    "i4 = np.array([one,one])\n",
    "\n",
    "output = []\n",
    "\n",
    "#case 1\n",
    "out1 = network_1(i1)\n",
    "print(\"case One - output \", out1)\n",
    "if out1 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 2\n",
    "out2 = network_1(i2)\n",
    "print(\"case Two - output \", out2)\n",
    "if out2 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 3\n",
    "out3 = network_1(i3)\n",
    "print(\"case Three - output \", out3)\n",
    "if out3 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 4\n",
    "out4 = network_1(i4)\n",
    "print(\"case Four - output \", out4)\n",
    "if out4 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "print(\"Final Output  \", output)\n",
    "    \n",
    "#Print Final Network Weights\n",
    "print(\"Final Network Weights \", network_1.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Limitations of Hebbian Learning rule\n",
    "\n",
    "## 3a. \n",
    "Can you learn the AND gate using 2 neurons in the output layer instead of one? If yes, describe what changes you might need to make to your algorithm in 2b. If not, explain why not, and what consequences it might entail for the use of hebbian learning for complex real-world tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3a. \n",
    "Instead of one neuron in the output layer, we can also learn the AND gate with two neurons. We also have to use negative weights if two neurons are used. As a result, we can encode 0 as -0.5 and 1 as 0.5  So, negative weights are also considered. However, in a real-world scenario, this would be very computationally heavy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. \n",
    "Train the network using hebbian learning for AND gate with the same arguments as defined in 2d. but now multiply the number of epochs by 20. Can your network still learn AND gate correctly? Inspect the initial and final network weights, and compare them against the network weights in 2d. Based on this, explain your observations for the network behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights  [[0.0588866  0.00584288]]\n",
      "Testing \n",
      "zero  [1 0 1 0 1]\n",
      "one  [0 1 1 1 1]\n",
      "case One - output  [0.6]\n",
      "case Two - output  [1.]\n",
      "case Three - output  [1.]\n",
      "case Four - output  [0.8]\n",
      "Final Output   [1, 1, 1, 1]\n",
      "Final Network Weights  [[4.5388866  4.48584288]]\n"
     ]
    }
   ],
   "source": [
    "#Implementation for 3b. (same as 2d. but with change of one argument)\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension) / 10\n",
    "\n",
    "print(\"initial weights \", input_2_output_weight)\n",
    "network_2 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "hebbian(network_2, train_data, lr=2e-2, epochs=400)\n",
    "\n",
    "#Test the trained network and print the network output for all 4 cases.\n",
    "print(\"Testing \")\n",
    "\n",
    "zero = np.random.choice([1,0], snn_timestep, p=[0.3, 0.7])\n",
    "print(\"zero \", zero)\n",
    "\n",
    "one = np.random.choice([1,0], snn_timestep, p=[0.7, 0.3])\n",
    "print(\"one \", one)\n",
    "    \n",
    "i1 = np.array([zero,zero])\n",
    "i2 = np.array([zero,one])\n",
    "i3 = np.array([one,zero])\n",
    "i4 = np.array([one,one])\n",
    "\n",
    "output = []\n",
    "\n",
    "#case 1\n",
    "out1 = network_2(i1)\n",
    "print(\"case One - output \", out1)\n",
    "if out1 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 2\n",
    "out2 = network_2(i2)\n",
    "print(\"case Two - output \", out2)\n",
    "if out2 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 3\n",
    "out3 = network_2(i3)\n",
    "print(\"case Three - output \", out3)\n",
    "if out3 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 4\n",
    "out4 = network_2(i4)\n",
    "print(\"case Four - output \", out4)\n",
    "if out4 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "print(\"Final Output  \", output)\n",
    "    \n",
    "#Print Final Network Weights\n",
    "print(\"Final Network Weights \", network_2.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3b. \n",
    "The network over-saturated the weights, which leads to producing wrong results time and again. This is because the limitation of Hebbian learning does not accommodate negative learning. the final weights of the network drastically increase compared to 2b .As we run it for 400 epochs the weights increase to very large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. \n",
    "Based on your observations and response in 3b., can you explain another limitation of hebbian learning rule w.r.t. weight growth? Can you also suggest a possible remedy for it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3c. \n",
    "Oja's rule is an extension of the Hebbian learning rule. In the classical Hebbian learning rule, the update scheme for weights may result in very large weights when the number of iterations is large.\n",
    "Oja's rule is based on normalized weights, and the weights are normally normalized to unit length. This simple change results in a different but more general and stable weight update scheme compared to the classical Hebbian learning scheme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. \n",
    "To resolve the issues with hebbian learning, one possibility is Oja's rule. In this exercise, you will implement and train an SNN using Oja's learning rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oja(network, train_data, lr=1e-5, epochs=10):\n",
    "    \"\"\" \n",
    "    Function to train a network using Hebbian learning rule\n",
    "        Args:\n",
    "            network (SNN): SNN network object\n",
    "            train_data (list): training data \n",
    "            lr (float): learning rate\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples. \n",
    "        \n",
    "        Write the operations required to compute the weight increment according to the hebbian learning rule. Then increment the network weights. \n",
    "    \"\"\"\n",
    "    \n",
    "    #iterate over the epochs\n",
    "    for ee in range(epochs):\n",
    "        w = np.zeros((1,2))\n",
    "        #iterate over all samples in train_data\n",
    "        for data in train_data:\n",
    "            \n",
    "            #compute the firing rate for the input\n",
    "            r1 = np.sum(data[0][0])/len(data[0][0])\n",
    "            r2 = np.sum(data[0][1])/len(data[0][1])\n",
    "\n",
    "            #compute the firing rate for the output\n",
    "            ro = np.sum(data[1])/len(data[1])\n",
    "\n",
    "            #compute the correlation using the firing rates calculated above\n",
    "            cor1 = r1 * ro\n",
    "            cor2 = r2 * ro\n",
    "            \n",
    "            oja_1 = network.input_2_output_connection.weights[0][0] * ro * ro\n",
    "            oja_2 = network.input_2_output_connection.weights[0][1] * ro * ro\n",
    "\n",
    "            #compute the weight increment\n",
    "            w[0][0] = lr * (cor1 - oja_1)\n",
    "            w[0][1] = lr * (cor2 - oja_2)\n",
    "            \n",
    "            #increment the weight\n",
    "            network.input_2_output_connection.weights += w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your implementation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights  [[0.00205112 0.03298693]]\n",
      "Testing \n",
      "zero  [0 1 0 1 0]\n",
      "one  [0 1 1 1 1]\n",
      "case One - output  [0.4]\n",
      "case Two - output  [0.4]\n",
      "case Three - output  [0.4]\n",
      "case Four - output  [0.8]\n",
      "Final Output   [0, 0, 0, 1]\n",
      "Final Network Weights  [[0.30063955 0.32346698]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_1 = np.array([i1,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_2 = np.array([i2,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_3 = np.array([i3,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_4 = np.array([i4,one])\n"
     ]
    }
   ],
   "source": [
    "#Define a variable for input dimension\n",
    "input_dimension = 2\n",
    "\n",
    "#Define a variable for output dimension\n",
    "output_dimension = 1\n",
    "\n",
    "#Define a variable for voltage decay\n",
    "vdecay = 0.5\n",
    "\n",
    "#Define a variable for voltage threshold\n",
    "vth = 0.5\n",
    "\n",
    "#Define a variable for snn timesteps\n",
    "snn_timestep = 5\n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension) / 10\n",
    "\n",
    "#print the initial weights\n",
    "print(\"initial weights \", input_2_output_weight)\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "network_3 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Get the training data for AND gate using the function defined in 2a. \n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "#Train the network using the function defined in 3d. with the appropriate arguments\n",
    "oja(network_3, train_data, lr=2e-2, epochs=10)\n",
    "\n",
    "#Test the trained network and print the network output for all 4 cases.\n",
    "print(\"Testing \")\n",
    "\n",
    "zero = np.random.choice([1,0], snn_timestep, p=[0.3, 0.7])\n",
    "print(\"zero \", zero)\n",
    "\n",
    "one = np.random.choice([1,0], snn_timestep, p=[0.7, 0.3])\n",
    "print(\"one \", one)\n",
    "    \n",
    "i1 = np.array([zero,zero])\n",
    "i2 = np.array([zero,one])\n",
    "i3 = np.array([one,zero])\n",
    "i4 = np.array([one,one])\n",
    "\n",
    "output = []\n",
    "\n",
    "#case 1\n",
    "out1 = network_3(i1)\n",
    "print(\"case One - output \", out1)\n",
    "if out1 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 2\n",
    "out2 = network_3(i2)\n",
    "print(\"case Two - output \", out2)\n",
    "if out2 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 3\n",
    "out3 = network_3(i3)\n",
    "print(\"case Three - output \", out3)\n",
    "if out3 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 4\n",
    "out4 = network_3(i4)\n",
    "print(\"case Four - output \", out4)\n",
    "if out4 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "print(\"Final Output  \", output)\n",
    "    \n",
    "#Print Final Network Weights\n",
    "print(\"Final Network Weights \", network_3.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Spike-time dependent plasticity (STDP)\n",
    "\n",
    "## 4a. \n",
    "What is the limitation with hebbian learning that STDP aims to resolve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4a. \n",
    "In Hebbian learning -- an association is strengthened when two events happen at around the same time (temporal order is not essential). STDP tries to solve this by decreasing weights. In Spike-timing-dependent plasticity (STDP) -- an association is strengthened when two events happen in the temporal order. Otherwise, it’s weakened. This is a timing (order) dependent specialization of Hebbian learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. \n",
    "Describe the algorithm to train a network using STDP learning rule. You do not need to describe encoding here. Your algorithm should be such that its naturally translatable to a program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4b. \n",
    "Spike-Timing-Dependent-Plasticity (STDP) is a biologically plausible learning mechanism that self-learns synaptic weights based on the degree of temporal correlations between the pre-and post-synaptic spike events.  The pre-synaptic trace when the pre-synaptic spike arrives and exponentially decays over time. Hence, the pre-synaptic trace encodes the timing correlation between pre- and post-neuronal spikes in the positive timing window. The strength (weight) of the synapse is potentiated if a pre-synaptic spike triggers the post-neuron within a period of time that is determined by a threshold,. The synaptic weight is depressed for larger spike timing differences. The STDP weight updates are applied to the synapses only at the time instances of post-synaptic firing. Specifically, we use the weight-dependent positive-STDP rule whose characteristic is formulated as follows. where Δw is the change in the synaptic weight, The amount of weight change has a non-linear dependence on the current weight (w), which is specified by the product of (wmax-w) and (w-wmin). Smaller the absolute value of the current weight, the larger the ensuing weight change and vice versa .Such nonlinear weight-dependent updates ensure a gradual increase (decrease) of the synaptic weight toward the maximum (minimum) bound, thereby improving the efficiency of synaptic feature learning. Note that the synaptic weights are locally updated in an unsupervised way based on the spiking behaviors of pre-/post-neurons at adjacent layers.In convolutional SNNs, the weight kernels locally inter-connecting the successive layers stride over the pre-neuronal maps to construct the output feature maps at every time step. In an event of a post-spike, the time difference between corresponding pre- and post-neuronal spikes is used to conduct individual STDP updates on the convolutional weights. In case of multiple post-neuronal spikes in an output feature map, averaged STDP updates are applied to the kernel weights. Accordingly, the STDP learning enables the weight kernels to self-learn useful features from the complex input patterns. In addition to performing STDP updates on the weight kernels, we modulate the firing threshold of the units in the corresponding feature map to enable kernels (among the feature maps in a convolutional layer) to learn different representations of input patterns. In the event of a post-neuronal spike in a convolutional feature map, we uniformly increase the firing threshold of all the post-units constituting the feature map. In the period of non-firing, the firing threshold of the feature map exponentially decays over time. In this work, we exploit the feature learning capabilities of STDP learning for appropriately initializing the convolutional weights and corresponding neuronal firing thresholds in multi-layer systems. We greedily pre-train each convolutional layer one at a time using the unsupervised STDP learning and uniform threshold adaptation scheme. We begin by training the first convolutional layer that enables the weight kernels to discover low-level characteristic features from input patterns in an unsupervised manner. At every time step, the convolutional kernels slide over the input maps to detect the characteristic features and construct output feature maps. The unit in output feature maps fires when the convolutional kernel captures the characteristic features, and the weight kernel is updated with STDP and the threshold adaptation mechanism. After the first convolutional layer is trained, the adjusted weight kernels and neuronal firing thresholds are frozen to feed the input again for estimating the average firing rate of units in the output feature maps. The generated feature maps of the first convolutional layer (the nonlinear transformations of inputs) are spatially pooled and passed to the next convolutional layer to extract the higher-level representations in hierarchical models. This process is repeated until all convolutional layers are pre-trained. Note that we do not modify the synaptic weights of the fully-connected layer (or the classifier) during the pre-training procedure. Therefore, the unsupervised pre-training mechanism, in essence, initially finds out unique features and underlying structures of input patterns for the task at hand prior to supervised fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. \n",
    "In this exercise, you will implement the STDP learning algorithm to train a network. STDP has many different flavors. For this exercise, we will use the learning rule defined in: https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.330110021. Pay special attention to Equations 2 and 3. \n",
    "\n",
    "Below is the class definition for STDP learning algorithm. Your task is to fill in the components so that the weights are updated in the right manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP():\n",
    "    \"\"\"Train a network using STDP learning rule\"\"\"\n",
    "    def __init__(self, network, A_plus, A_minus, tau_plus, tau_minus, lr, snn_timesteps=20, epochs=30, w_min=0, w_max=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            network (SNN): network which needs to be trained\n",
    "            A_plus (float): STDP hyperparameter\n",
    "            A_minus (float): STDP hyperparameter\n",
    "            tau_plus (float): STDP hyperparameter\n",
    "            tau_minus (float): STDP hyperparameter\n",
    "            lr (float): learning rate\n",
    "            snn_timesteps (int): SNN simulation timesteps\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples.  \n",
    "            w_min (float): lower bound for the weights\n",
    "            w_max (float): upper bound for the weights\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.A_plus = A_plus\n",
    "        self.A_minus = A_minus\n",
    "        self.tau_plus = tau_plus\n",
    "        self.tau_minus = tau_minus\n",
    "        self.snn_timesteps = snn_timesteps\n",
    "        self.lr = lr\n",
    "        self.time = np.arange(0, self.snn_timesteps, 1)\n",
    "        self.sliding_window = np.arange(-4, 4, 1) #defines a sliding window for STDP operation. \n",
    "        self.epochs = epochs\n",
    "        self.w_min = w_min\n",
    "        self.w_max = w_max\n",
    "    \n",
    "    def update_weights(self, t, i):\n",
    "        \"\"\"\n",
    "        Function to update the network weights using STDP learning rule\n",
    "        \n",
    "        Args:\n",
    "            t (int): time difference between postsynaptic spike and a presynaptic spike in a sliding window\n",
    "            i(int): index of the presynaptic neuron\n",
    "        \n",
    "        Fill the details of STDP implementation\n",
    "        \"\"\"\n",
    "        #compute delta_w for positive time difference\n",
    "        if t>0:\n",
    "            delta_w = self.A_plus * np.exp(-t / self.tau_plus)\n",
    "\n",
    "        #compute delta_w for negative time difference\n",
    "        else:\n",
    "            delta_w = -self.A_minus * np.exp(-t / self.tau_minus)\n",
    "\n",
    "        #update the network weights if weight increment is negative\n",
    "        if delta_w < 0:\n",
    "            change = self.lr * delta_w * (self.network.input_2_output_connection.weights - self.w_min)\n",
    "            self.network.input_2_output_connection.weights += change \n",
    "\n",
    "        #update the network weights if weight increment is positive\n",
    "        elif delta_w > 0:\n",
    "            change = self.lr * delta_w * (self.w_max - self.network.input_2_output_connection.weights)\n",
    "            self.network.input_2_output_connection.weights += change \n",
    "            \n",
    "    def train_step(self, train_data_sample):\n",
    "        \"\"\"\n",
    "        Function to train the network for one training sample using the update function defined above. \n",
    "        \n",
    "        Args:\n",
    "            train_data_sample (list): a sample from the training data\n",
    "            \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        input = train_data_sample[0]\n",
    "        output = train_data_sample[1]\n",
    "        for t in self.time:\n",
    "            if output[t] == 1:\n",
    "                for i in range(2):\n",
    "                    for t1 in self.sliding_window:\n",
    "                        if (0<= t + t1 < self.snn_timesteps) and (t1!=0) and (input[i][t+t1] == 1):\n",
    "                            self.update_weights(t1, i)\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"\n",
    "        Function to train the network\n",
    "        \n",
    "        Args:\n",
    "            training_data (list): training data\n",
    "        \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        for ee in range(self.epochs):\n",
    "            for train_data_sample in training_data:\n",
    "                self.train_step(train_data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights  [[0.03654227 0.04913749]]\n",
      "Testing \n",
      "zero  [0 0 0 0 0]\n",
      "one  [1 1 1 1 0]\n",
      "case One - output  [0.]\n",
      "case Two - output  [0.4]\n",
      "case Three - output  [0.4]\n",
      "case Four - output  [0.8]\n",
      "Final Output   [0, 0, 0, 1]\n",
      "Final Network Weights  [[0.36095549 0.36095549]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:27: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_1 = np.array([i1,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:28: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_2 = np.array([i2,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_3 = np.array([i3,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_9814/3475826507.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_4 = np.array([i4,one])\n"
     ]
    }
   ],
   "source": [
    "#Define a variable for input dimension\n",
    "input_dimension = 2\n",
    "\n",
    "#Define a variable for output dimension\n",
    "output_dimension = 1\n",
    "\n",
    "#Define a variable for voltage decay\n",
    "vdecay = 0.5\n",
    "\n",
    "#Define a variable for voltage threshold\n",
    "vth = 0.5\n",
    "\n",
    "#Define a variable for snn timesteps\n",
    "snn_timestep = 5\n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension) / 10\n",
    "print(\"initial weights \", input_2_output_weight)\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "network_4 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Get the training data for AND gate using the function defined in 2a. \n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "#Create an object of STDP class with appropriate arguments\n",
    "STDP_obj = STDP(network_4, A_plus = 0.6, A_minus = 0.3, tau_plus = 8, tau_minus = 5, lr = 0.25, \n",
    "                snn_timesteps=5, epochs=30, w_min=0, w_max=1)\n",
    "\n",
    "#Train the network using STDP\n",
    "STDP_obj.train(train_data)\n",
    "\n",
    "#Test the trained network and print the network output for all 4 cases.\n",
    "print(\"Testing \")\n",
    "\n",
    "zero = np.random.choice([1,0], snn_timestep, p=[0.3, 0.7])\n",
    "print(\"zero \", zero)\n",
    "\n",
    "one = np.random.choice([1,0], snn_timestep, p=[0.7, 0.3])\n",
    "print(\"one \", one)\n",
    "    \n",
    "i1 = np.array([zero,zero])\n",
    "i2 = np.array([zero,one])\n",
    "i3 = np.array([one,zero])\n",
    "i4 = np.array([one,one])\n",
    "\n",
    "output = []\n",
    "\n",
    "#case 1\n",
    "out1 = network_4(i1)\n",
    "print(\"case One - output \", out1)\n",
    "if out1 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 2\n",
    "out2 = network_4(i2)\n",
    "print(\"case Two - output \", out2)\n",
    "if out2 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 3\n",
    "out3 = network_4(i3)\n",
    "print(\"case Three - output \", out3)\n",
    "if out3 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 4\n",
    "out4 = network_4(i4)\n",
    "print(\"case Four - output \", out4)\n",
    "if out4 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "print(\"Final Output  \", output)\n",
    "    \n",
    "#Print Final Network Weights\n",
    "print(\"Final Network Weights \", network_4.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: OR Gate\n",
    "Can you train the network with the same architecture in Q2-4 for learning the OR gate. You will need to create another function called genORTrainData. Then create an SNN and train it using STDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your implementation of genORTrainData here. \n",
    "def genORTrainData(snn_timestep):\n",
    "    \"\"\" \n",
    "    Function to generate the training data for AND \n",
    "        Args:\n",
    "            snn_timestep (int): timesteps for SNN simulation\n",
    "        Return:\n",
    "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
    "        \n",
    "        Write the expressions for encoding 0 and 1. Then append all 4 cases of AND gate to the list train_data\n",
    "    \"\"\"\n",
    "    \n",
    "    #Initialize an empty list for train data\n",
    "    train_data = []\n",
    "    \n",
    "    #encode 0. Numpy random choice function might be useful here. \n",
    "    zero=list(np.random.choice(2,snn_timestep,0.7))\n",
    "\n",
    "    \n",
    "    #encode 1. Numpy random choice function might be useful here. \n",
    "    one=list(np.random.choice(2,snn_timestep,0.3))\n",
    "\n",
    "    #Append all 4 cases of AND gate to train_data. Numpy stack operation might be useful here.\n",
    "  \n",
    "    i1 = np.array([zero,zero])\n",
    "    i2 = np.array([zero,one])\n",
    "    i3 = np.array([one,zero])\n",
    "    i4 = np.array([one,one])\n",
    "    \n",
    "    case_1 = np.array([i1,zero])\n",
    "    case_2 = np.array([i2,one])\n",
    "    case_3 = np.array([i3,one])\n",
    "    case_4 = np.array([i4,one])\n",
    "    train_data = np.stack((case_1, case_2, case_3, case_4))\n",
    "    \n",
    "    return train_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_1 = np.array([i1,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_2 = np.array([i2,one])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_3 = np.array([i3,one])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_4 = np.array([i4,one])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[array([[1, 0, 0, 1, 0],\n",
       "               [1, 0, 0, 1, 0]]), list([1, 0, 0, 1, 0])],\n",
       "       [array([[1, 0, 0, 1, 0],\n",
       "               [0, 1, 0, 0, 0]]), list([0, 1, 0, 0, 0])],\n",
       "       [array([[0, 1, 0, 0, 0],\n",
       "               [1, 0, 0, 1, 0]]), list([0, 1, 0, 0, 0])],\n",
       "       [array([[0, 1, 0, 0, 0],\n",
       "               [0, 1, 0, 0, 0]]), list([0, 1, 0, 0, 0])]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=genORTrainData(5)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial weights  [[0.0104821  0.07226297]]\n",
      "Testing \n",
      "zero  [0 0 0 1 1]\n",
      "one  [1 1 1 1 1]\n",
      "case one - output  [0.4]\n",
      "case Two - output  [0.6]\n",
      "case Three - output  [0.6]\n",
      "case Four - output  [1.]\n",
      "Final Output   [0, 1, 1, 1]\n",
      "Final Network Weights  [[0.4701283 0.4701283]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:30: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_1 = np.array([i1,zero])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:31: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_2 = np.array([i2,one])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:32: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_3 = np.array([i3,one])\n",
      "/var/folders/lb/cs19dljn0ljgxp1gsgzk4lgr0000gn/T/ipykernel_97378/875808090.py:33: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  case_4 = np.array([i4,one])\n"
     ]
    }
   ],
   "source": [
    "#Train the network for OR gate here using the implementation from 4c. \n",
    "\n",
    "#Define a variable for input dimension\n",
    "input_dimension = 2\n",
    "\n",
    "#Define a variable for output dimension\n",
    "output_dimension = 1\n",
    "\n",
    "#Define a variable for voltage decay\n",
    "vdecay = 0.5\n",
    "\n",
    "#Define a variable for voltage threshold\n",
    "vth = 0.5\n",
    "\n",
    "#Define a variable for snn timesteps\n",
    "snn_timestep = 5\n",
    "\n",
    "#Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension) / 10\n",
    "print(\"initial weights \", input_2_output_weight)\n",
    "\n",
    "#Initialize an snn using the arguments defined above\n",
    "network_5 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "#Get the training data for AND gate using the function defined in 2a. \n",
    "training_data = genORTrainData(snn_timestep)\n",
    "\n",
    "#Create an object of STDP class with appropriate arguments\n",
    "OR_obj = STDP(network_5, A_plus = 0.6, A_minus = 0.3, tau_plus = 8, tau_minus = 5, lr = 0.25, \n",
    "                snn_timesteps=5, epochs=30, w_min=0, w_max=1)\n",
    "\n",
    "#Train the network using STDP\n",
    "OR_obj.train(training_data)\n",
    "\n",
    "#Test the trained network and print the network output for all 4 cases.\n",
    "print(\"Testing \")\n",
    "\n",
    "zero = np.random.choice([1,0], snn_timestep, p=[0.3, 0.7])\n",
    "print(\"zero \", zero)\n",
    "\n",
    "one = np.random.choice([1,0], snn_timestep, p=[0.7, 0.3])\n",
    "print(\"one \", one)\n",
    "    \n",
    "i1 = np.array([zero,zero])\n",
    "i2 = np.array([zero,one])\n",
    "i3 = np.array([one,zero])\n",
    "i4 = np.array([one,one])\n",
    "\n",
    "output = []\n",
    "\n",
    "#case 1\n",
    "out1 = network_5(i1)\n",
    "print(\"case one - output \", out1)\n",
    "if out1 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 2\n",
    "out2 = network_5(i2)\n",
    "print(\"case Two - output \", out2)\n",
    "if out2 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 3\n",
    "out3 = network_5(i3)\n",
    "print(\"case Three - output \", out3)\n",
    "if out3 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "\n",
    "#case 4\n",
    "out4 = network_5(i4)\n",
    "print(\"case Four - output \", out4)\n",
    "if out4 > vth:\n",
    "    output.append(1)\n",
    "else:\n",
    "    output.append(0)\n",
    "print(\"Final Output  \", output)\n",
    "    \n",
    "#Print Final Network Weights\n",
    "print(\"Final Network Weights \", network_5.input_2_output_connection.weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
